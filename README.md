

<!--$$
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
$$-->

# Naive Bayes Spam Email Classifier

- [TLDR](https://github.com/nripstein/Bayes-email-4/edit/main/README.md#mathematical-steps)
- [How to Use](https://github.com/nripstein/Bayes-email-4/edit/main/README.md#mathematical-steps)
- [Mathematical Steps](https://github.com/nripstein/Bayes-email-4/edit/main/README.md#mathematical-steps)
- [Have an Email? Check if it's Spam](https://github.com/nripstein/Bayes-email-4/edit/main/README.md#have-an-email-check-if-its-spam)
- [Classifier Accuracy Analysis and Visualizations](https://github.com/nripstein/Bayesian-Email-Classifier/edit/main/README.md#classifier-accuracy-analysis-and-visualizations)
- [Future Directions](https://github.com/nripstein/Bayes-email-4/edit/main/README.md#future-directions)

## TLDR

This program classifies emails as spam or not spam.  It uses a naïve bayes classifier algorithm, which is a machine learning algorithm.  It does not use any machine learning libraries, rather, I designed and customized the algorithm using Bayesian inference.

## Why I Started This Project
In a Bayesian statistic class, I learned about the naive Bayes classifier and its applications in text classification. I was intrigued by the algorithm's simplicity and effectiveness, so I decided to implement it from scratch in Python without using any machine learning libraries. I wanted to challenge myself to understand the underlying math and logic of the algorithm, and to gain hands-on experience in building machine learning models from scratch.

Currently, I'm also learning how to create machine learning algorithms using neural networks in TensorFlow. I hope to come back to this project at some point to compare the performance of the naive Bayes classifier with a neural network classification on the same task. Overall, this project is a stepping stone for me to explore the world of machine learning and deepen my understanding of the algorithms behind it.

## How to use:
This section is incomplete

## Mathematical steps:

Preparation for training:
-	The body section of each email’s is converted to a set of words, which is added to a column called “word_set” using the string_to_word_set() function.
-	The dataset is split into a training and testing data set.

Training the data:
-	A new data frame is created which contains the frequency at which each word in the training data set appears in the “spam” and “ham” category.

Classifying the data:
-	The Bayesian posterior probability that an email is spam is determined according to the following (basic) procedure:

### Bayesian posterior calculation

Hypotheses:  
H1: The email is spam
H2: The email is ham

Prior probabilities: P(category)  
P(spam) and P(ham). We treat these as P(spam) = P(ham) = 0.5. In the strictest mathematical sense, P(spam) should be the overall frequency of spam emails, but I want to reduce bias, and the training dataset has much more spam than a normal email adress.

Likelihood calculations: P(words|category)  
$$P(\textrm{words}|\textrm{spam}) = \prod_{i=1}^{n} \frac{\textrm{freq of word}_i \textrm{in spam}}{\textrm{ham words}}$$

$$P(\textrm{words}|\textrm{ham}) = \prod_{i=1}^{n} \frac{\textrm{freq of word}_i \textrm{in ham}}{\textrm{ham words}}$$

Posteriors:

$$ P(H1|D) = \frac{P(D|H1)P(H1)}{P(D|H1)P(H1) + P(D|H2)P(H2)} $$

$$ P(\textrm{spam}|\textrm{words}) = \frac{P(\textrm{words}|\textrm{spam})P(\textrm{spam})}{P(\textrm{words}|\textrm{spam})P(\textrm{spam}) + P(\textrm{words}|\textrm{ham})P(\textrm{ham})} $$

Deviations from standard procedure:
-	If, when doing a likelihood calculstion, a word appears in “spam” but not "ham," then it is treated as if it's been in ham 0.5 times.
-	If a word appears in "ham" but not "spam," then it is treated as if it has appeared 0.01 tines

The posterior probability of each email being spam is computed.  If it's determined that there's a greater than 90% probability of the email being spam, then it is classified as spam


## Have an Email? Check if it's Spam
This section is incomplete.  Features coming soon
- I will impliment a feature where you can paste an email into a txt file and the program will tell you the probability that it is spam, and print out the words colour coded according to a heatmap for how likely it is to be spam
- Example of usage on spam email example generated by chatGPT
<img width="1259" alt="Colour Email Example" src="https://user-images.githubusercontent.com/98430636/220796858-b3ce8667-1215-448a-94af-fcda1074dc5d.png">


## Classifier Accuracy Analysis and Visualizations
To evaluate the performance of the Naive Bayes spam email classifier, I trained it on 15,000 emails and tested it on 2,000. The overall accuracy of the model was 92.05%, which seems like a promising result. The confusion matrix below shows the number of true positives, true negatives, false positives, and false negatives for the test set.


### Confusion Matrix
<img src="https://user-images.githubusercontent.com/98430636/220796903-e92a632e-a977-4ffc-bb74-c94f31e747b9.png" alt="confusion matrix" width="70%">


### Common Spam Words
![spam_ratio](https://user-images.githubusercontent.com/98430636/220796954-f5bfe2d8-a98b-40f3-91ae-5c2b37af3dda.png)

### Common Ham Words
![ham_ratio](https://user-images.githubusercontent.com/98430636/220796981-ae2880dc-37cd-4715-9413-91203022eabc.png)




## Future Directions
This section is incomplete
- If an email is one long word, my algorithm doesn't really know how to treat it. I'd like to do something about that

![image](https://user-images.githubusercontent.com/98430636/219830713-955e4862-a03e-414b-a746-72d83dea6699.png)
